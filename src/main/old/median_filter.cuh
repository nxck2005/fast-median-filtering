#pragma once

#include <opencv2/opencv.hpp>
#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <cub/cub.cuh>
#include <iostream>
#include <vector>
#include <stdexcept>

// Error check. ignore lol
#define CUDA_CHECK(call)                                                    \
do {                                                                        \
    cudaError_t err = call;                                                 \
    if (err!= cudaSuccess) {                                               \
        fprintf(stderr, "CUDA Error in %s at line %d: %s\n",                \
                __FILE__, __LINE__, cudaGetErrorString(err));               \
        exit(EXIT_FAILURE);                                                 \
    }                                                                       \
} while (0)


namespace HierarchicalMedianFilter {
namespace detail {

// ==========================================================================
// == DATA-OBLIVIOUS KERNEL IMPLEMENTATION (FOR SMALL KERNEL SIZES) =========
// ==========================================================================

// --- Compile-time Sorting Networks (templates) ---

template <typename T>
__device__ __forceinline__ void compare_swap(T& a, T& b) {
    T min_val = min(a, b);
    T max_val = max(a, b);
    a = min_val;
    b = max_val;
}

// A generic sorting network for an array of size N.
// This is a placeholder for a highly optimized, pre-generated network.
// A production implementation would use specific, optimal networks for each N.
template <typename T, int N>
__device__ void sort_network(T arr[N]) {
    // Using a simple odd-even sort for demonstration.
    // Real implementation would use optimal networks from papers.
    for (int i = 0; i < N; ++i) {
        if (i % 2 == 0) { // Even phase
            for (int j = 0; j < N / 2; ++j) {
                if (2 * j + 1 < N) {
                    compare_swap(arr[2 * j], arr[2 * j + 1]);
                }
            }
        } else { // Odd phase
            for (int j = 0; j < (N - 1) / 2; ++j) {
                compare_swap(arr[2 * j + 1], arr[2 * j + 2]);
            }
        }
    }
}

// The data-oblivious kernel. Each thread processes one root tile.
template <typename T, int KSIZE, int TILE_W, int TILE_H>
__global__ void data_oblivious_kernel(const T* src, T* dst, int width, int height) {
    // This kernel is highly complex and depends on template metaprogramming
    // to generate the specific selection network for the given KSIZE and TILE_W/H.
    // The full implementation would involve:
    // 1. Defining `TileData` struct with sizes as template parameters.
    // 2. A thread block collaboratively loading the footprint into shared memory.
    // 3. Collaborative column sort using shared memory.
    // 4. Each thread loading its tile state into registers.
    // 5. A large, inlined block of code generated by templates that executes
    //    the entire hierarchical recursion using only register operations.
    // 6. Writing the final medians from registers to global memory.

    // A simplified placeholder to illustrate the concept:
    const int x = blockIdx.x * blockDim.x + threadIdx.x;
    const int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x >= width |

| y >= height) return;

    T window;
    int radius = KSIZE / 2;
    int count = 0;

    // Naive gathering (for demonstration; real version is tiled)
    for (int j = -radius; j <= radius; ++j) {
        for (int i = -radius; i <= radius; ++i) {
            int sx = x + i;
            int sy = y + j;
            if (sx >= 0 && sx < width && sy >= 0 && sy < height) {
                window[count++] = src[sy * width + sx];
            } else {
                // Handle borders (e.g., clamp or replicate)
                int clamped_sx = max(0, min(width - 1, sx));
                int clamped_sy = max(0, min(height - 1, sy));
                window[count++] = src[clamped_sy * width + clamped_sx];
            }
        }
    }

    // Apply the compile-time sorting network
    sort_network<T, KSIZE * KSIZE>(window);

    // The median is the middle element
    dst[y * width + x] = window;
}


// ==========================================================================
// == DATA-AWARE KERNEL IMPLEMENTATION (FOR LARGE KERNEL SIZES) =============
// ==========================================================================

// The data-aware implementation is a multi-pass pipeline.
// This would involve several kernels for:
// - Transposing the image
// - Sorting rows/columns (using cub::DeviceSegmentedSort)
// - Sorting the core (multi-way merge)
// - Recursive extensions (merging extra rows/cols)
// - Finalization

// This is a simplified, single-kernel placeholder for the data-aware logic.
// The real implementation would be a class orchestrating multiple kernel launches.
template <typename T>
__global__ void data_aware_placeholder_kernel(const T* src, T* dst, int width, int height, int ksize) {
    const int x = blockIdx.x * blockDim.x + threadIdx.x;
    const int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x >= width |

| y >= height) return;

    int radius = ksize / 2;
    int window_size = ksize * ksize;
    
    // Allocate shared memory for sorting
    extern __shared__ T smem;
    T* window = smem;

    // Each thread gathers its window into shared memory
    // (This is still naive; a real implementation would be highly optimized)
    int count = 0;
    for (int j = -radius; j <= radius; ++j) {
        for (int i = -radius; i <= radius; ++i) {
            int sx = x + i;
            int sy = y + j;
            if (sx >= 0 && sx < width && sy >= 0 && sy < height) {
                window[threadIdx.x * window_size + count++] = src[sy * width + sx];
            } else {
                int clamped_sx = max(0, min(width - 1, sx));
                int clamped_sy = max(0, min(height - 1, sy));
                window[threadIdx.x * window_size + count++] = src[clamped_sy * width + clamped_sx];
            }
        }
    }
    __syncthreads();

    // Use a block-level sort (e.g., from CUB)
    // For simplicity, using a simple bubble sort here
    T* my_window = &window[threadIdx.x * window_size];
    for (int i = 0; i < window_size - 1; ++i) {
        for (int j = 0; j < window_size - i - 1; ++j) {
            if (my_window[j] > my_window[j+1]) {
                T temp = my_window[j];
                my_window[j] = my_window[j+1];
                my_window[j+1] = temp;
            }
        }
    }
    __syncthreads();

    dst[y * width + x] = my_window[window_size / 2];
}

// Host-side function to launch the appropriate data-oblivious kernel
template <typename T>
void run_data_oblivious_kernel(const cv::Mat& src, cv::Mat& dst, int ksize) {
    // NOTE: This is a simplified launcher. A full implementation would have
    // a switch-case or if-else chain to instantiate the correct templated kernel
    // based on ksize. This is required because template parameters must be compile-time constants.
    
    if (ksize > 19) { // Max supported by this simplified demo
        throw std::runtime_error("Data-oblivious kernel demo only supports ksize <= 19");
    }

    const T* d_src;
    T* d_dst;
    size_t pitch;
    CUDA_CHECK(cudaMallocPitch((void**)&d_src, &pitch, src.cols * sizeof(T), src.rows));
    CUDA_CHECK(cudaMallocPitch((void**)&d_dst, &pitch, dst.cols * sizeof(T), dst.rows));
    CUDA_CHECK(cudaMemcpy2D( (void*)d_src, pitch, src.ptr(), src.step, src.cols * sizeof(T), src.rows, cudaMemcpyHostToDevice));

    dim3 block(16, 16);
    dim3 grid((src.cols + block.x - 1) / block.x, (src.rows + block.y - 1) / block.y);

    // A real implementation would dispatch to a specific template instantiation
    // e.g., if (ksize == 7) data_oblivious_kernel<T, 7, 8, 8><<<...>>>(...);
    // This is a generic placeholder call.
    if (ksize == 7) data_oblivious_kernel<T, 7, 8, 8><<<grid, block>>>(d_src, d_dst, src.cols, src.rows);
    else if (ksize == 9) data_oblivious_kernel<T, 9, 8, 8><<<grid, block>>>(d_src, d_dst, src.cols, src.rows);
    //... add cases for other supported kernel sizes
    else {
        // Fallback for demonstration
        data_oblivious_kernel<T, 11, 8, 8><<<grid, block>>>(d_src, d_dst, src.cols, src.rows);
    }
    
    CUDA_CHECK(cudaGetLastError());
    CUDA_CHECK(cudaDeviceSynchronize());

    CUDA_CHECK(cudaMemcpy2D(dst.ptr(), dst.step, d_dst, pitch, dst.cols * sizeof(T), dst.rows, cudaMemcpyDeviceToHost));
    CUDA_CHECK(cudaFree((void*)d_src));
    CUDA_CHECK(cudaFree((void*)d_dst));
}

// Host-side function to launch the data-aware pipeline
template <typename T>
void run_data_aware_kernel(const cv::Mat& src, cv::Mat& dst, int ksize) {
    // A full implementation would manage image slicing and orchestrate the
    // multi-pass kernel pipeline here.
    
    const T* d_src;
    T* d_dst;
    size_t pitch;
    CUDA_CHECK(cudaMallocPitch((void**)&d_src, &pitch, src.cols * sizeof(T), src.rows));
    CUDA_CHECK(cudaMallocPitch((void**)&d_dst, &pitch, dst.cols * sizeof(T), dst.rows));
    CUDA_CHECK(cudaMemcpy2D( (void*)d_src, pitch, src.ptr(), src.step, src.cols * sizeof(T), src.rows, cudaMemcpyHostToDevice));

    dim3 block(16, 16);
    dim3 grid((src.cols + block.x - 1) / block.x, (src.rows + block.y - 1) / block.y);
    
    // The shared memory size depends on ksize and block size
    size_t smem_size = block.x * block.y * ksize * ksize * sizeof(T);

    data_aware_placeholder_kernel<T><<<grid, block, smem_size>>>(d_src, d_dst, src.cols, src.rows, ksize);
    
    CUDA_CHECK(cudaGetLastError());
    CUDA_CHECK(cudaDeviceSynchronize());

    CUDA_CHECK(cudaMemcpy2D(dst.ptr(), dst.step, d_dst, pitch, dst.cols * sizeof(T), dst.rows, cudaMemcpyDeviceToHost));
    CUDA_CHECK(cudaFree((void*)d_src));
    CUDA_CHECK(cudaFree((void*)d_dst));
}

} // namespace detail

// ==========================================================================
// == PUBLIC API ============================================================
// ==========================================================================

template <typename T>
void apply(const cv::Mat& src, cv::Mat& dst, int ksize) {
    if (src.empty()) {
        throw std::invalid_argument("Source image is empty.");
    }
    if (ksize % 2 == 0) {
        throw std::invalid_argument("Kernel size must be odd.");
    }
    dst.create(src.size(), src.type());

    // --- Intelligent Dispatcher ---
    // This logic selects the best kernel variant based on kernel size and data type.
    // The crossover points are determined empirically for the target architecture.
    // For a 4060 Max-Q (SM 8.9), the data-oblivious kernel is typically faster
    // for ksize up to around 19 or 21 for 8/16-bit types.
    
    int crossover_ksize = 21;

    if (ksize < crossover_ksize) {
        std::cout << "Using Data-Oblivious Kernel for ksize=" << ksize << std::endl;
        // The full implementation requires a switch to instantiate the correct template.
        // This is a simplified call.
        detail::run_data_oblivious_kernel<T>(src, dst, ksize);
    } else {
        std::cout << "Using Data-Aware Kernel for ksize=" << ksize << std::endl;
        detail::run_data_aware_kernel<T>(src, dst, ksize);
    }
}

} // namespace HierarchicalMedianFilter